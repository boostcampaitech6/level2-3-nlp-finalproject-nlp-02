import os
from collections import Counter
from typing import List, Tuple

import torch
from tqdm import tqdm
from transformers import PreTrainedTokenizer


class GECToRDataset:
    def __init__(
        self,
        srcs: List[str],
        d_labels: List[List[int]] = None,
        labels: List[List[int]] = None,
        word_masks: List[List[int]] = None,
        tokenizer: PreTrainedTokenizer = None,
        max_length: int = 128,
    ):
        self.tokenizer = tokenizer
        self.srcs = srcs
        self.d_labels = d_labels
        self.labels = labels
        self.word_masks = word_masks
        self.max_length = max_length
        self.label2id = None
        self.d_label2id = None

    def __len__(self):
        return len(self.srcs)

    def __getitem__(self, idx):
        src = self.srcs[idx]
        d_labels = self.d_labels[idx]
        labels = self.labels[idx]
        wmask = self.word_masks[idx]
        encode = self.tokenizer(
            src,
            return_tensors="pt",
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            is_split_into_words=True,
        )
        return {
            "input_ids": encode["input_ids"].squeeze(),
            "attention_mask": encode["attention_mask"].squeeze(),
            "d_labels": torch.tensor(d_labels).squeeze(),
            "labels": torch.tensor(labels).squeeze(),
            "word_masks": torch.tensor(wmask).squeeze(),
        }

    def append_vocab(self, label2id, d_label2id):
        self.label2id = label2id
        self.d_label2id = d_label2id
        for i in range(len(self.labels)):
            self.labels[i] = [
                self.label2id.get(l, self.label2id["<OOV>"]) for l in self.labels[i]
            ]
            self.d_labels[i] = [self.d_label2id[l] for l in self.d_labels[i]]

    def get_labels_freq(self, exluded_labels: List[str] = []):
        assert self.labels is not None and self.d_labels is not None
        flatten_labels = [
            ll for l in self.labels for ll in l if ll not in exluded_labels
        ]
        flatten_d_labels = [
            ll for l in self.d_labels for ll in l if ll not in exluded_labels
        ]
        return Counter(flatten_labels), Counter(flatten_d_labels)


def align_labels_to_subwords(
    srcs: List[str],
    word_labels: List[List[str]],
    tokenizer: PreTrainedTokenizer,
    batch_size: int = 100000,
    max_length: int = 128,
    keep_label: str = "$KEEP",
    pad_token: str = "<PAD>",
    correct_label: str = "$CORRECT",
    incorrect_label: str = "$INCORRECT",
):
    itr = list(range(0, len(srcs), batch_size))
    subword_labels = []
    subword_d_labels = []
    word_masks = []
    for i in tqdm(itr):
        encode = tokenizer(
            srcs[i : i + batch_size],
            max_length=max_length,
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            is_split_into_words=True,
        )
        for i, wlabels in enumerate(word_labels[i : i + batch_size]):
            d_labels = []
            labels = []
            wmask = []
            word_ids = encode.word_ids(i)
            previous_word_idx = None
            for word_idx in word_ids:
                if word_idx is None:
                    labels.append(pad_token)
                    d_labels.append(pad_token)
                    wmask.append(0)
                elif word_idx != previous_word_idx:
                    l = wlabels[word_idx]
                    labels.append(l)
                    wmask.append(1)
                    if l != keep_label:
                        d_labels.append(incorrect_label)
                    else:
                        d_labels.append(correct_label)
                else:
                    labels.append(pad_token)
                    d_labels.append(pad_token)
                    wmask.append(0)
                previous_word_idx = word_idx
            subword_d_labels.append(d_labels)
            subword_labels.append(labels)
            word_masks.append(wmask)
    return subword_d_labels, subword_labels, word_masks


def load_gector_format(
    input_file: str,
    delimeter: str = "SEPL|||SEPR",
    additional_delimeter: str = "SEPL__SEPR",
):
    srcs = []
    word_level_labels = (
        []
    )  # the size will be (#sents, seq_length) if not get_interactive_tags,
    # (#iteration, #sents, seq_length) if get_interactive_tags
    with open(input_file) as f:
        for line in f:
            src = [x.split(delimeter)[0] for x in line.split()]
            labels = [x.split(delimeter)[1] for x in line.split()]
            # Use only first tags. E.g. $REPLACE_meSEPL__SEPR$APPEND_too â†’ $REPLACE_me
            labels = [l.split(additional_delimeter)[0] for l in labels]
            srcs.append(src)
            word_level_labels.append(labels)
    return srcs, word_level_labels


def load_dataset(
    input_file: str,
    tokenizer: PreTrainedTokenizer,
    delimeter: str = "SEPL|||SEPR",
    additional_delimeter: str = "SEPL__SEPR",
    batch_size: int = 50000,  # avoid too heavy computation in the tokenization
    max_length: int = 128,
):
    srcs, word_level_labels = load_gector_format(
        input_file, delimeter=delimeter, additional_delimeter=additional_delimeter
    )
    d_labels, labels, word_masks = align_labels_to_subwords(
        srcs,
        word_level_labels,
        tokenizer=tokenizer,
        batch_size=batch_size,
        max_length=max_length,
    )
    return GECToRDataset(
        srcs=srcs,
        d_labels=d_labels,
        labels=labels,
        word_masks=word_masks,
        tokenizer=tokenizer,
        max_length=max_length,
    )
